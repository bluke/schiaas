\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{acronym}
\acrodef{IaaS}{Infrastructure as a Service}
\acrodef{SaaS}{Software as a Service}
\acrodef{PaaS}{Platform as a Service}
\acrodef{VM}{Virtual Machine}
\acrodef{PM}{Physical Machine}
\acrodef{BTU}{billing time unit}
\acrodef{EC2CU}{EC2 Compute Unit}
\acrodef{HPC}{High Performance Computing}
\acrodef{unistra}{University of Strasbourg}
\acrodef{rv}[RV]{random variable}
\acrodef{pdf}[PDF]{probability density function}
\acrodef{cdf}[CDF]{cumulative distribution function}


\title{Modeling the accuracy of Monte-Carlo approach for Cloud based workflow simulations.}
\author{\IEEEauthorblockN{Luke~Bertot 
			and Stéphane~Genaud 
			and Julien~Gossa}
	\IEEEauthorblockA{Icube-ICPS --- UMR 7357, Univeristé de Strasbourg, CNRS\\
		P\^ole API Blvd S. Bant, 67400 Illkirch\\
		email: \url{lbertot@unistra.fr}, \url{gossa@unistra.fr}, \url{genaud@unistra.fr}}
	}



\begin{document}

\maketitle

\begin{abstract}
  The pay as you go model of  cloud operators makes exceeding the arbitrary time
  limits of the payment model costly. Budgeting the cost of running a scientific
  workflow  requires users  to  be able  to reliably  predict  runtimes of  jobs
  within. Attempts  to do so  are hampered by  the heterogeneity and  opacity of
  cloud  platform   which  make   job  runtimes  both   variable  and   hard  to
  predict. Variable jobs call for a  form of stochastic simulation, which can be
  achieved  through  either complex  calculations  heavily  constrained to  some
  reference cases or  through a Monte-Carlo approach  iterating on deterministic
  simulations. In this paper we study  the limits of the Monte-Carlo approach by
  characterising the relationship  between the precision of  the input variables
  and the simulations results, as well as the impact of the number of iterations
  for each level of input precision. We show.
\end{abstract}

\begin{IEEEkeywords}
cloud computing, infrastructure as a service, simulation, montecarlo.
\end{IEEEkeywords}

\section{Introduction}

Whether to  model impractical  experiments or extract  information out  of large
quantities of data, large scale computing is central to scientific, and sometime
industrial,  operations.  Institutions  have  historically taken  the burden  of
providing the computing power necessary for the research of its members, usually
through  the acquisition  of a  cluster  or the  formation  of a  grid with  its
existing resources, sometime pooling resource  with other institution to achieve
higher computing power.  These resources are made available freely to members of
the institutions within the constraints  of time sharing and available computing
power.

Over the  last decade the advancement  of virtualization techniques has  lead to
the emergence of new economic and exploitation approach of computer resources in
the form of \ac{IaaS}. In this model, all computing resources are made available
on demand  by third-party operators  and payed based  on usage.  The  ability to
provision resources on demand provided by  \ac{IaaS} operators is mainly used in
two ways.  Firstly,  for scaling purposes where new machines  are brought online
to fulfill  service availability in  the face of  higher load, this  approach is
used for  providing service allows for  a lower baseline cost  while still being
able to deal  by spikes in demand  by provisioning machine on  the go.  Secondly
for parallelizing tasks to achieve shorter makespan as equal cost, this approach
is  used for  scientific and  industrial  workload with  a clear  end and  where
runtime is heavily dependent on computing power.  This approach is made possible
by   the   pricing   model   of  cloud   infrastructure,   as   popularized   by
AWS\footnote{Amazon  Web  Services},  in  which  payment  for  computing  power,
provided  as  \acp{VM},  happens  in  increment of  arbitrary  length  of  time,
\ac{BTU}, usually of one hour. Running two \ac{VM} side by side for one \ac{BTU}
each costs  the same  as running  one \ac{VM}  for two  \ac{BTU}, but  every BTU
started is  owed in full.  As  such within a  workflow a slowed job  forcing the
subsequent job to run beyond the \ac{BTU}  limit can cause a full \ac{BTU} to be
invoiced for a handful of seconds of computation. Cases where such a thing might
not  always  be  avoided  but  \ac{IaaS}  to  be  reliably  used  in  scientific
computations  the eventuality  of  an  overrun must  be  reliably predicted  and
budgeted.

Accurate  prediction of  the  runtime  of scientific  workloads  is hampered  by
multiple factors. First \ac{IaaS} operates in a opaque fashion, the exact nature
of the underlying platforms are unknown and extremely heterogeneous as operators
complete   their   data-centers   over   the  years   with   new   servers   and
equipment.  Secondly  cloud  systems  are   multi-tenant  by  nature  which  ads
uncertainty due to  contention on network and memory accesses,  depending on how
\ac{VM}  are scheduled  and  the activity  of  your \emph{neighbors}.  Even when
\ac{IaaS} operators attempt to mask these irregularities in computing power and
network access by guaranteeing a minimum performance, variability occurs in
presence of \emph{less-than-noisy-neighbors} as it is not in the interest of the
\ac{IaaS} operators to limit power when available over the guaranteed minimums.
These factor add to the already hight difficulty of modeling job exaction times
on Networks of Computers as shown by Lastovetsky and Twamley in
\cite{Lastovetsky05}.

To deal with the inherent unpredictability introduced by opaque heterogeneous
girds, the standard approach is to consider jobs runtimes to be stochastic.
Every jobs is modeled by a \ac{rv} that models the whole spectrum of possible
runtimes incurred by the heterogeneity of the platform or the noisy neighbors,
or even the slight variation on the number of cache misses incurred during the
execution of the application. Stochastic simulations are quite harder than
deterministic ones, where deterministic simulation give a single numerical
result, stochastic simulation results in \ac{rv} whose parameters are defined by
the \ac{rv}. It as been shown in \cite{Li97} and \cite{Ludwig01} that assuming
\acp{rv} are independent, the \ac{pdf} of the \ac{rv} resulting from to job
following each other is the convolution of the job's \acp{pdf}, whereas in the
cases of jobs merging the \ac{cdf} of the final \ac{rv} is the product of the
merging \acp{rv} \acp{cdf}.  The constant switching between \ac{pdf} and
\ac{cdf} makes computation of such \acp{rv} impractical. And methods to solve
rapidly such simulation are heavily reliant on simplification.

Monte Carlo simulations rely on repeated sampling of input \acp{rv} to compute a
sufficient number of outcomes to obtain a good approximation of the output
\ac{rv}. This method sidesteps the computational complexity of pure stochastic
methods replacing it by sheer number repeated deterministic simulations. In our
case we create multiple realisations by drawing a walltime for each jobs using
it's respective \ac{rv}, each realisation is then passed on to a simulator
computing the makespan of that realisation, the makespans are finally aggregated
in an makespan \ac{rv} that is the result of the simulation. This final \acl{rv}
can be used to derive confidence intervals of the time or number of BTU taken by
the execution of the workflow.

Monte Carlo methods well studied from a mathematics stand point, accuracy of the
method can be bounded and in some case improved upon using quasi-random series.

\texttt{TODO move the last 3 paragraph elsewhere}

\section{Related Works}

Monte Carlo have been suggested as a work around for stochastic workflow
resolutions as far as 1963 in \cite{beenhaker63} and for computer science as far
as 1971 for network analysis by \emph{Burt et al.} in \cite{burt71}. In
\cite{Canon10}, \emph{Canon} and \emph{Jeannot} choose to use Monet Carlo
simulations to try optimize DAG schedules for heterogeneous environments. {DAG}
scheduling is also the problem at the core \emph{Zheng} and \emph{Sakellariou}
in \cite{Zheng13}. ElasticSim\cite{cai16} is an \ac{IaaS} Monte Carlo simulator
created as an extension to the CloudSim\cite{cloudsim} simulator. In this paper
we will work on workflows executed by the \emph{Schlouder}\cite{Michon17} cloud
broker by \emph{Michon et al.}. Our simulator \emph{SimSchlouder}\cite{?}  is
based on SimGrid \cite{simgrid} platform developed by \emph{Quinson et al.}.

\texttt{TODO Should explaintions of stochastic sims be here with thier quotes, should
more details be given to each ref content?}

\section{Problem description}

\bibliographystyle{IEEEtran}
\bibliography{montecarlo-simulation}

\newpage
\begin{verbatim}
* Difficulties of simulation
	- opaque platforms
	- difficulties simulation precision (aka opaque codes)(cite simgrid ?)
	- difficulties of stochastic simulation (cite robust DAG over possible approach)(cite elastic sim)
	- introduce Montecarlo (just lipservice)

! Related works
	- Robust DAG by Jeanot et al : does the full run around of stochastic simulation approach (back-cite the other ?) -> wanted to test weather scheduling was robust
	- Elastic sim use montecarlo processes also for DAG	
	- Stochatsict Dag Sechduling  Zheng et Sakellariou -> looks for the best scheduling in a runtime scheduling contexti
	? Simgrid
	- Schlouder
	? Towards a realistic performance model Lastovesky Rychkov -> but program execution variability.

! Problem description
	
* Montecarlo as a workaround 
	- Deterministic simulator can be turned stochastic by being run on a sample of stochastic inputs

* Deterministic simulators are easy to evaluate they have 1 correct result for any set of input 


* The stochastic component must be evaluated
	- incorrect sample of inputs in the simulator will lead to wrong results
	- how does the precision of the input affects the precision of the results
	- how does the precision of the input affects the montecarlo process (ie. num of simulations necessary and convergence speed)

! Methodology 

* Rerunning experiments and simulator validation
	- the schlouder Cloud batch scheduler
	- simschlouder and how it relates to schlouder
	- the experimental backlog (OMSSA/Montage and the platforms on with they ran

* the perfect model
	- model based on real run values
	- draw interval centered on real runtime

! Results
\end{verbatim}


\end{document}
