\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{tikz}
\usetikzlibrary{fit}

\usepackage{acronym}
\acrodef{IaaS}{Infrastructure as a Service}
\acrodef{SaaS}{Software as a Service}
\acrodef{PaaS}{Platform as a Service}
\acrodef{dag}[DAG]{directed acyclic graph}
\acrodef{VM}{Virtual Machine}
\acrodef{PM}{Physical Machine}
\acrodef{BTU}{billing time unit}
\acrodef{EC2CU}{EC2 Compute Unit}
\acrodef{HPC}{High Performance Computing}
\acrodef{unistra}{University of Strasbourg}
\acrodef{rv}[RV]{random variable}
\acrodef{pdf}[PDF]{probability density function}
\acrodef{cdf}[CDF]{cumulative distribution function}
\acrodef{mcs}[MCS]{Monte-Carlo simulation}
\acrodefplural{mcs}[MCS's]{Monte-Carlo simulations}

\newcommand*\rot{\rotatebox{90}}
\newcommand{\pmpc}[1]{$\pm#1\%$}
\newcommand{\etal}[1]{\emph{#1 et al.}}
\newcommand{\pc}[1]{$#1\%$}

\title{Modeling the accuracy of Monte-Carlo approach for Cloud based workflow simulations.}
\author{\IEEEauthorblockN{Luke~Bertot 
			and Stéphane~Genaud 
			and Julien~Gossa}
	\IEEEauthorblockA{Icube-ICPS --- UMR 7357, Univeristé de Strasbourg, CNRS\\
		P\^ole API Blvd S. Bant, 67400 Illkirch\\
		email: \url{lbertot@unistra.fr}, \url{genaud@unistra.fr}, \url{gossa@unistra.fr}}
	}



\begin{document}

\maketitle

\begin{abstract}
  In the  cloud computing  model, cloud providers  invoice clients  for resource
  consumption. Hence, tools helping the client to budget the cost of running its
  application are  of pre-eminent  importance. To  that end,  a number  of cloud
  simulators have been  proposed by researchers. However, the  attempts to reach
  reliable predictions  are hampered by  the heterogeneity and opacity  of cloud
  platform which make job runtimes both  variables and hard to predict. Variable
  runtimes call for a form of stochastic simulation.  
%
  We  consider the  execution of  batch  of jobs  which can  be bag-of-tasks  or
  workflow.   The execution  is handled  by our  broker Schlouder  in charge  of
  provisioning the infrastructure and scheduling  the jobs with regard to their
  estimated runtimes specified  by the user.  While we are  able to simulate how
  the  broker handles  executions  with our  tool  SimSchlouder, this  simulator
  cannot predict the possible variability in  makespan and cost.  In this paper,
  we propose  to enrich  simulation with  the Monte-Carlo  method.  We  study on
  practical  examples  the  relationship  between the  precision  of  the  input
  variables  and  the  accuracy  of   the  simulation  results.   We  show  that
  imprecisions in runtimes input by the  user are largely amortized in the final
  resulting makespan, which  varies in about a third of  the inputs variability.
  Finally we show our method can in most cases correctly captures real makespans. 
 %Finally, we  compare how  well a set  of real executions  are captured  by our
 %method.
\end{abstract}

\begin{IEEEkeywords}
cloud computing, computer simulation, monte carlo methods.
\end{IEEEkeywords}

\section{Introduction.}

Whether to  model impractical  experiments or extract  information out  of large
quantities of data, large scale computing is central to scientific, and sometime
industrial,  operations.  Institutions  have  historically taken  the burden  of
providing the computing power necessary for the research of its members, usually
through  the acquisition  of a  cluster  or the  formation  of a  grid with  its
existing resources, sometime pooling resource  with other institution to achieve
higher computing power.  These resources are made available freely to members of
the institutions within the constraints  of time sharing and available computing
power.

Over the  last decade the advancement  of virtualization techniques has  lead to
the emergence of new economic and exploitation approach of computer resources in
the form of \ac{IaaS}. In this model, all computing resources are made available
on demand  by third-party operators  and payed based  on usage.  The  ability to
provision resources on demand provided by  \ac{IaaS} operators is mainly used in
two ways.  Firstly,  for scaling purposes where new machines  are brought online
to fulfill  service availability in  the face of  higher load, this  approach is
used for  providing service allows for  a lower baseline cost  while still being
able to deal  by spikes in demand  by provisioning machine on  the go.  Secondly
for parallelizing tasks to achieve shorter makespan as equal cost, this approach
is  used for  scientific and  industrial  workload with  a clear  end and  where
runtime is heavily dependent on computing power.  This approach is made possible
by   the   pricing   model   of  cloud   infrastructure,   as   popularized   by
AWS\footnote{Amazon  Web  Services},  in  which  payment  for  computing  power,
provided  as  \acp{VM},  happens  in  increment of  arbitrary  length  of  time,
\ac{BTU}, usually of one hour. Running two \ac{VM} side by side for one \ac{BTU}
each costs  the same  as running  one \ac{VM}  for two  \ac{BTU}, but  every BTU
started is  owed in full.  As  such within a  workflow a slowed job  forcing the
subsequent job to run beyond the \ac{BTU}  limit can cause a full \ac{BTU} to be
invoiced for a handful of seconds of computation. Cases where such a thing might
not  always  be  avoided  but  \ac{IaaS}  to  be  reliably  used  in  scientific
computations  the eventuality  of  an  overrun must  be  reliably predicted  and
budgeted.

Accurate  prediction of  the  runtime  of scientific  workloads  is hampered  by
multiple factors. First \ac{IaaS} operates in a opaque fashion, the exact nature
of the underlying platforms are unknown and extremely heterogeneous as operators
complete  their data-centers  over the  years  with new  servers and  equipment.
Secondly cloud systems  are multi-tenant by nature which ads  uncertainty due to
contention  on  network  and  memory  accesses, depending  on  how  \ac{VM}  are
scheduled  and  the activity  of  your  \emph{neighbors}.  Even  when  \ac{IaaS}
operators attempt  to mask these  irregularities in computing power  and network
access by guaranteeing a minimum  performance, variability occurs in presence of
\emph{less-than-noisy-neighbors} as it  is not in the interest  of the \ac{IaaS}
operators to  limit power  when available over  the guaranteed  minimums.  These
factor add  to the already  high difficulty of  modeling job execution  times on
Networks  of  Computers  as   shown  by  \emph{Lastovetsky}  and  \emph{Twamley}
in~\cite{Lastovetsky05}.

To deal  with the inherent  unpredictability introduced by  opaque heterogeneous
platforms, the standard approach is to  consider jobs runtimes to be stochastic.
Every job can be modeled by a \ac{rv} that models the whole spectrum of possible
runtimes. These  \ac{rv} are  the basis required  for a  stochastic simulations.
Such  simulations   output  a  random   variable  of  the   observed  phenomenon
(\emph{makespan}  or  \emph{\ac{BTU}}) which  in  turn  can  be used  to  create
intervals of possible  results with their assorted confidence. In  this paper we
will evaluate the precision of Monte Carlo method based stochastic simulators in
the context of cloud scheduling.

\section{Related Works.}

Computing the resulting \ac{rv} of a stochastic simulation is a non-trivial
process.  \acp{rv} are defined through their \ac{pdf} and \ac{cdf} where $CDF(x)
= \int_{-\infty}^{x} PDF(y) dy$. Works by \etal{Li}~\cite{Li97} and
\etal{Ludwig}\cite{Ludwig01} show that in the context of task \ac{dag} with
independent \acp{rv} the \ac{rv} representing successive tasks is a convolution
product of their respective \acp{pdf} whereas the \ac{rv} of parallel tasks
joining is the product of their respective \acp{cdf}. Solving stochastic
\ac{dag} numerically is therefore extremely computationally intensive even when
the initial constraint of independence of the \acp{rv} can be fulfilled. In the
general cases solving the \ac{rv} for a \ac{dag} is deemed a \#P-complete
problem, though some approximation can be used to attempt and solve it
numerically as in~\cite{dodin85}.

To sidestep the computational difficulty \emph{van Slyke} suggests in
1963\cite{Slyke63} the use of Monte Carlo methods. In \acp{mcs} the random
inputs are repeatedly sampled to compute multiple results, those results are
used to compute the process overall \acl{rv}. Because it is based on a
repetition of a deterministic simulation using samples drawn for the input
\ac{rv}, this method avoids complex operations such as convolution or the
repeated integration needed to switch from an \ac{rv}'s \ac{pdf} to its
\ac{cdf}. This approach was used in 1971 by~\etal{Burt} for network
analysis~\cite{burt71} and in the context of task scheduling by
\etal{Canon}~\cite{Canon10} to compare the robustness of \ac{dag} schedules, and
\etal{Zheng}, to optimize schedules stochastic \acp{dag}. \etal{Cai} have worked
to extend CloudSim~\cite{cloudsim} with ElasticSim, a Monte Carlo
simulator~\cite{cai16}. As \acp{mcs} gain in traction we want to
evaluate the quality of this approach relatively to the precision of the tasks
models.

\section{Work Context}

\subsection{Schlouder and SimSchlouder}
In the recent years, we have developped a client-side job broker for IaaS clouds
called Schlouder~\cite{Michon17}. This broker is able to submit on behalf of its
user a  batch of  jobs, be  it a  set of  independent jobs  or a  workflow.  The
broker's main role is to schedule the set of jobs onto a set of cloud resources,
which the broker can  scale up or down depending on  a \emph{strategy} chosen by
the user.  Technically, the broker connects  to the cloud management system (for
instance OpenStack) to  instruct how the infrastrustrure  should be provisionned
and  then assigns  the jobs  to  the resources  using the  Slurm job  management
system.  Schlouder offers two main strategies for job submission:
\begin{itemize}
\item ASAP (or \textit{as soon as possible}): schedules the job immediately onto
  an already started VM, or if  no resource is available, immediately provisions
  a new VM to start the job.  Hence this strategy tries to minimize the makespan
  of the job batch execution.

\item AFAP (or \textit{as full as  possible}): schedules each job in priority to
  an already  available resource where  the remaining time  before a new  BTU is
  started is greater or equal to the job's estimated runtime. By solving this 
 bin-packing problem, this strategy tends to minimize the monetary cost of the
 batch execution.
\end{itemize}

A follow-up of this work has been  SimSchlouder, a simulator able to predict the
behavior of Schlouder using ASAP or AFAP. Integrated to Schlouder, the simulator
allows  the user  to  request an  estimation  of the  cost  and makespan  before
choosing a strategy for a real execution.  SimSchlouder is implemented on top of
SimGrid~\cite{simgrid}, a discrete  event simulator aimed at  studying a variety
of distributed systems, such as Grids,  Clouds or HPC systems. SimGrid and hence
SimSchlouer takes as  input i) a textual description of  the hardware components
and their characteristics,  ii) a program describing  the application processes,
and using its models for computation and communications, outputs a chronology of
the various  events. Notice that  the simulation  result is deterministic  for a
given infrastructure description and application.


\subsection{Experimental Set-up and Test Applications}
\label{sc:setup}

Our experimental evaluation is carried out on two applications having very
different characteristics.
\begin{itemize}
	\item Montage\cite{montage2009}, the Montage Astronomical Image Mosaic
		Engine, is designed to splice astronomical images. As shown in
		Fig.~\ref{fig:montage} this application is a fork-join type
		workflow. This application is extremely data intensive with a
		\emph{communication-to-computation} ratio superior to $90\%$.
	\item OMSSA\cite{Geer2004}, the Open Mass-Spectrometry Search Algorithm, is
		used to analyze mass-spectrometer results. The application is
                parallelized as a bag-of-tasks, \textit{i.e} as a number of
                independent parallel jobs. This application is
		computation intensive with a
		\emph{communication-to-computation} ratio under to $20\%$.
\end{itemize}


\begin{figure}
	\resizebox{0.5\textwidth}{!}{%
		\input{gfx/montage.tex}
		}%
	\caption{Montage workflow diagram.}
	\label{fig:montage}
\end{figure}

We run these two applications with Schlouder on a private cloud.  We set up a 96
core cloud based on four identical  dual $2.67GHz$ Intel Xeon X5650 servers with
KVM based virtualization  and an Openstack cloud-front (first  2012.1, and later
2014.2). This cluster was exclusive to our usage and special attention was given
to never overload the system.


\subsection{Monte-Carlo Simulaton}

Even in a stable environment devoid  of cross-chatter like our private platform,
the  length of  execution  of jobs  within  the Montage  or  OMSSA workflows  is
noticeably  variable. This  can not  be reflected  directly through  simulation,
which produces  deterministic results.  The unpredictability  of these variation
means that a job's runtime is best modeled by a random variable which requires a
stochastic simulation.  To simulate an unpredictable phenomenon a \ac{mcs} draws
repeatedly inputs from a random variable modeling the phenomenon in order to get
result \emph{samples}  which are then  aggregated as the \ac{mcs}'s  result.  In
our case, the random variable models what  values the job runtimes can take, and
the result samples are the makespans obtained through simulation of the workload
execution.  An important  question is how should we define  the random variables
to reflect variablility in the job  runtimes. In the method described hereafter,
we make the  assumption that the random value take  its values \emph{around} the
runtimes  specified by  the  user and  we  will study  the  effects of  choosing
different extents  around the specified  runtimes for the random  variables (see
section~\ref{sec:mcs-accuracy}. Let us now describe the \ac{mcs} process itself.



%\paragraph{Method} 
Suppose an  application consisting of  $n$ jobs,  independent or organized  as a
workflow.   The  simulation engine  SimSchlouder  takes  as input  the  runtimes
$T_1, \ldots ,  T_n$ of the different  jobs as specified by  the user, schedules
the jobs and  finally outputs the makespan  $M$ of the whole  batch.  The user's
estimation of  jobs' runtimes might be  inaccurate, and we describe  hereafter a
method to characterize the impact of repeated imprecisions (on each job runtime)
on the  final makespan.  We  have developed a \ac{mcs}  tool to compute a
confidence interval around the makespan produced   by   simulation.   Let   us
sketch   the   overall   process   (see Fig~\ref{fig:mc-process}):
\begin{itemize} 
\item from  the set of user  specified runtimes $\{T_i\}$, the  system generates
  $s$ sets of perturbed runtimes  $\{T_i^1\}, \ldots, \{T_i^s\}$. Each perturbed
  runtime    is   a    random   value    uniformly   drawn    in   the    range
  $[T_i (1-P); T_i  (1+P)]$, where $P$ is called  the \emph{perturbation level},
  e.g  5\%.  We call \emph{realization}  each  such random  draw  of  a set  of
  perturbed runtimes.
\item a simulation  is run for each realization, producing a makespan $M^i$
	sample for each,
\item  a  normal  distribution  ${\cal N}(\mu,\sigma)$  is  fitted  on
	the different  makespan values.
\end{itemize}
\begin{figure}
	\centering
	\resizebox{0.5\textwidth}{!}{%
		\input{gfx/mc-process.tex}
		}
                \caption{Overview   of   the   Monte-Carlo   process   :   $500$
                  realizations   are  generated   by   drawing   and  adding   a
                  perturbation to each  job's runtime of the  user provided set,
                  every simulation  is then  simulated, the  resulting makespans
		  samples are fitted into the final result. \label{fig:mcprocess}
		  }
      \label{fig:mc-process}
\end{figure}
Fitting is done to a normal distribution because, in essence the makespan is the
sum  of the  runtimes of  the jobs  on the  critical path  of the  schedule.  To
measure the impact  of the runtime perturbation on the  makespan, we compute the
range $[\mu-2\sigma;\mu+2\sigma]$ (that is  a 95\% confidence interval) relative
to  the mean  $\mu$. 

\section{Evaluation}

In this section, we evaluate the  proposed method from two viewpoints. The first
one regards the method itself. We  study the relationship between inputs and the
confidence interval for makespan (section~\ref{sec:mcs-accuracy}), how fast the
method construct a satisfactory interval (section~\ref{sec:mcs-convergence}), and
... (FIXME: fitting necessary?). The second viewpoint is the most interesting to
the user.  Given a set  of real executions on  the same inputs,  all potentially
showing slightly different  runtimes and makespans, we assess how  many of these
real observations  are captured in the  confidence interval build by  our method
(section~\ref{sc:rl}).
  


\subsection{Accuracy of the \acl{mcs}}
\label{sec:mcs-accuracy}

To be relevant  confidence intervals must remain as small  as possible all while
keeping  the  confidence  guaranteed.  Therefore   we  want  to  understand  the
relationship  between the  input  perturbation  and the  size  of the  resulting
confidence interval.  In this section we  look to establish how accuracy of the
input, the  job's runtimes, affects the accuracy of the \acp{mcs}  output, the
width  of the  95\%  confidence  interval.  To  establish  what relations  exist
between the  inputs perturbations and  the outputted confidence interval  we ran
\acp{mcs}  at different  perturbation  levels.   The \emph{perturbation  level},
expressed as  a percentage, defines  an interval  around each jobs  base runtime
were the job's  simulated runtimes can be  drawn -- e.g.~in a  simulation with a
\pmpc{15} perturbation level a job with a 100s base runtime can take any runtime
between  85s  and 115s.  These  \acp{mcs}  will be  based  on  6 different  real
executions  of  Montage, and  done  with  four different  perturbations  levels,
\pmpc{5}, \pmpc{10}, \pmpc{15} and, \pmpc{20}.   The \acp{mcs} are done with 500
simulations.

For each perturbation level we also simulate the \emph{best-case} where every
job takes the shortest runtime possible at this perturbation level and the
\emph{worst-case} where every job take as long as possible at this perturbation
level. These worst-case/best-case scenarios present the absolute limits of cases
possible for any given perturbation levels and their results are the endpoints
of a 100\% confidence interval for this perturbation level.

Tab.~\ref{tab:perts}. shows the relative results of those simulations.
For results of a \ac{mcs} the result shown is \[\frac{4\sigma}{\mu}\] the width
of the 95\% confidence interval divided by the average value. For
worst-case/best-case scenarios the result is computed as
\[\frac{|M_{W}-M_{B}|}{(M_{W}+M_{B})/2}\] the width of the 100\% confidence
interval divided by the arithmetic mean of it's endpoints.

\begin{table}
	\begin{tabular}{|l|cc|cc|}
		\hline
		Input&\multicolumn{2}{|c|}{$95\%$Confidence
		Interval}&\multicolumn{2}{c|}{Worst/Best Scenarios}\\
		Perturbations&AFAP&ASAP&AFAP&ASAP\\
		\hline
		\pmpc{5}&\pc{1.6}&\pc{6}--\pc{10}&\pc{9}&\pc{10}\\
		\pmpc{10}&\pc{3}&\pc{8}--\pc{20}&\pc{20}&\pc{18}--\pc{26}\\
		\pmpc{15}&\pc{5}&\pc{15}--\pc{20}&\pc{30}&\pc{27}--\pc{38}\\
		\pmpc{20}&\pc{6.6}&\pc{15}--\pc{20}&\pc{40}&\pc{37}--\pc{48}\\
		\hline
	\end{tabular}
	\caption{Precision of the Monte-Carlo simulations}
	\label{tab:perts}
\end{table}


Results are separated by scheduling strategy because simulations of different
strategies exhibited significantly different results. The strategy applied during
the simulation is always the same as the one used in the execution from which
the base times where obtained. Differences between results fall down
mostly to how the choice of strategy affects scheduling and how that in turn
affects how the simulation results. Because AFAP optimizes to limit the number
of core used, in our base dataset all jobs are run one after the other on a
single core, this means that i) any perturbation on any job affects the
makespan and ii) scheduling being nonexistent and does not affect the result.
Conversely because ASAP uses multiple cores to optimize makespan not every jobs
will affect the makespan. The notion of critical path comes into play and only
the jobs on the cores that took the longest to finish are a part of the critical
path. This is further more compounded by fork and join structure of the Montage
workflow (Fig.~\ref{fig:montage}), and by the SimSchlouder scheduler
which only schedules jobs once all dependencies have been executed, and will do
so depending on which runtimes of jobs already done. This means that in
experiments using the ASAP experiment, not every job is part of the critical
path but the critical path changes depending on the perturbation of each job.
The results show that workflows structure and jobs scheduling have a non
negligible impact on how perturbation affect the makespan's possible values.

The worst-case/best-case simulations behaved mostly as one would expect
them too. In AFAP where the makespan is the sum of every job's runtime the
makespan varies by two times the perturbation level. ASAP show more instability,
due to the potential change of scheduling and variation of the critical path,
but globally the makespan variation is close to two times the perturbation
level. This confirms that as a trade-off for it's simplicity the
worst-case/best-case simulation will give any additional precision that
what is nakedly obvious when looking at input precision. 

The \emph{\acl{mcs}} on the other hand result in ranges much more focused than
those given thought worst-case/best-case scenarios. When using the AFAP strategy the
\acp{mcs} give consistently extremely focused results. The $95\%$ confidence interval
relative size appears to be a third of the input perturbation level. This result
can partially be explained by the presence of every job on the critical path.
Because the perturbation for these job are drawn with the same space a large
number of the will tend to cancel out, this works well in montage because the
workflow contains large pools of similar jobs. When using ASAP with lower
perturbation levels the final $95\%$ confidence interval varies from half to the
full width observed in the worst/best cases scenarios.
This means the \ac{mcs} despite using more simulations did not bring
a significant improvement in precision. However when brought to higher level of
perturbation the \ac{mcs} shows a significant improvement.

\subsection{\acl{mcs} convergence and confidence}
\label{sec:mcs-convergence}

We saw previously that \aclp{mcs} generally give smaller confidence intervals
than a full worst/best cases scenarios would do. But the worst/best approach requires
only two easily generated simulation whereas a our \acp{mcs} required 500
simulations each. In this section we try examine the evolution of the previous
\acp{mcs} results depending on the number of performed simulation to figure how
many simulations are necessary, and if the \acp{mcs} perturbation levels affects
the number of simulations required.

In simple Monte-Carlo based computations, the accuracy of the result increases
as $\frac{1}{\sqrt{n}}$, where $n$ is the number of samples\cite{Press92}.
Our experiment includes both a non trivial simulation step, where
scheduling and job dependencies affects which runtimes are significant, and a
strong aggregation step using normal law fitting. 

To determine the convergence properties of our \ac{mcs} we observe the evolution
of the normal distribution obtained depending on the number of simulations done. 
Using the simulations order we are able to produce the fitting we would
have obtained after any arbitrary number of simulation. For every subset we
observe the absolute relative error in the resulting values of $\mu$ and $\sigma$ 
when compared to the value taken when all 500 simulation results are used. 

Tab.~\ref{tab:mcs-convergence}. shows how many simulations are required before the
absolute error consistently passes under the $.5\%$ threshold average makespan
value $\mu$ and the $1\%$ threshold of the quarter length of the confidence
interval $\sigma$ for different input perturbation levels. For the fitting
process to bear any value we started using at least 5 simulations. The result
show that for this type of simulations at reasonable level of perturbation $\mu$
converges quickly. A hundred simulation is generally enough to get a sense of
the workload makespan. On the other hand the computation of $\sigma$ is much
more volatile and does not appear to exhibit  as strong a convergence. This is
partly compensated by the fact the values of $\sigma$ small compared to the
values of $\mu$.

\begin{table}
	\centering
	%\caption{Monte Carlo simulation convergence based on perturbation}
	\begin{tabular}{|r|cc|cc|cc|cc|}
		\cline{2-9}
		\multicolumn{1}{c|}{}& \multicolumn{2}{c|}{$5\%$}& \multicolumn{2}{c|}{$10\%$}&\multicolumn{2}{c|}{$15\%$}& \multicolumn{2}{c|}{$20\%$}\\
		\multicolumn{1}{c|}{}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}\\
		\hline
		min&$>5$&$402$&$>5$&$423$&$>5$&$318$&$>5$&$353$\\
		mean&$14$&$448$&$30$&$465$&$29$&$409$&$73$&$442$\\
		max&$44$&$498$&$88$&$490$&$71$&$480$&$238$&$497$\\
		\hline
	\end{tabular}
	\caption{Monte Carlo simulation convergence}
	\label{tab:mcs-convergence}
\end{table}

\subsection{\acl{mcs} fitting process}

The fitting process used to aggregate the final result of the \ac{mcs} also 
possesses it's own confidence characteristic. The method we used
gives a standard deviation for the fitted value of $\mu$ and $\sigma$. Relative
standard deviation as a function of the number of the simulations are shown
Fig.~\ref{fig:confidence}. We see here that the standard deviation measured on
$\mu$ and $\sigma$ converges in following a $\frac{1}{\sqrt{n}}$ pattern
common observed in Monte-Carlo methods. These observations comfort us in our
choice of fitting results to a Normal law since they show adding more samples
consistently reduce standard deviation on the results estimates. We also see the
higher perturbation levels reduce the confidence in the estimated value of $\mu$
but doesn't affect significantly the confidence in the estimates of $\sigma$.
This is balanced by the fact the standard deviation on the estimate of $\mu$
is relatively small, whereas the standard deviation on the estimate of
$\sigma$, though unaffected by the input perturbation is significant in comparison
to the value of $\sigma$.

\begin{figure}
	\centering
	\resizebox{0.5\textwidth}{!}{%
		\input{gfx/meanconf.tex}
	}
	\resizebox{0.5\textwidth}{!}{%
		\input{gfx/sdconf.tex}
	}
	\caption{Evolution of confidence on $\mu$ and $\sigma$ depending on the
	number of simulations.}
	\label{fig:confidence}
\end{figure}


\subsection{Prediction of Real Executions}
\label{sec:rl}
  
We now examine how well real executions are captured by our method. If we
consider a specific application run, for which we observe the runtimes ${R_i}$ 
and a makespan.


 
In the method presented above, we establish the  relationship between  the user  specified
  inputs ${T_i}$ and the probable makespan, \textit{i.e} we want to evaluate how
  variability  of the  ${T_i}$ influences  the size  of the  confidence interval
  for the makespan. 
\begin{itemize}
\item  The second result concerns the relationship between real observations and 
the confidence interval for the makespan.  
\end{itemize}



%Simulation generally introduces a bias towards reality because simulation relies
%on a modeling  of the processes and platforms which  induces some simplification
%of the  reality. The method we  propose intends to construct  intervals in which
%real runtime observations will fall into, thus bluring the bias of simulation.


\bibliographystyle{IEEEtran}
\bibliography{montecarlo-simulation}

\newpage
\begin{verbatim}
* Difficulties of simulation
	- opaque platforms
	- difficulties simulation precision (aka opaque codes)(cite simgrid ?)
	- difficulties of stochastic simulation (cite robust DAG over possible approach)(cite elastic sim)
	- introduce Montecarlo (just lipservice)

! Related works
	- Robust DAG by Jeanot et al : does the full run around of stochastic simulation approach (back-cite the other ?) -> wanted to test weather scheduling was robust
	- Elastic sim use montecarlo processes also for DAG	
	- Stochatsict Dag Sechduling  Zheng et Sakellariou -> looks for the best scheduling in a runtime scheduling contexti
	? Simgrid
	- Schlouder
	? Towards a realistic performance model Lastovesky Rychkov -> but program execution variability.

! Problem description
	
* Montecarlo as a workaround 
	- Deterministic simulator can be turned stochastic by being run on a sample of stochastic inputs

* Deterministic simulators are easy to evaluate they have 1 correct result for any set of input 


* The stochastic component must be evaluated
	- incorrect sample of inputs in the simulator will lead to wrong results
	- how does the precision of the input affects the precision of the results
	- how does the precision of the input affects the montecarlo process (ie. num of simulations necessary and convergence speed)

! Methodology 

* Rerunning experiments and simulator validation
	- the schlouder Cloud batch scheduler
	- simschlouder and how it relates to schlouder
	- the experimental backlog (OMSSA/Montage and the platforms on with they ran

* the perfect model
	- model based on real run values
	- draw interval centered on real runtime

! Results
\end{verbatim}


\end{document}
