\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{acronym}
\acrodef{IaaS}{Infrastructure as a Service}
\acrodef{SaaS}{Software as a Service}
\acrodef{PaaS}{Platform as a Service}
\acrodef{dag}[DAG]{directed acyclic graph}
\acrodef{VM}{Virtual Machine}
\acrodef{PM}{Physical Machine}
\acrodef{BTU}{billing time unit}
\acrodef{EC2CU}{EC2 Compute Unit}
\acrodef{HPC}{High Performance Computing}
\acrodef{unistra}{University of Strasbourg}
\acrodef{rv}[RV]{random variable}
\acrodef{pdf}[PDF]{probability density function}
\acrodef{cdf}[CDF]{cumulative distribution function}


\newcommand{\etal}[1]{\emph{#1 et al.}}

\title{Modeling the accuracy of Monte-Carlo approach for Cloud based workflow simulations.}
\author{\IEEEauthorblockN{Luke~Bertot 
			and Stéphane~Genaud 
			and Julien~Gossa}
	\IEEEauthorblockA{Icube-ICPS --- UMR 7357, Univeristé de Strasbourg, CNRS\\
		P\^ole API Blvd S. Bant, 67400 Illkirch\\
		email: \url{lbertot@unistra.fr}, \url{gossa@unistra.fr}, \url{genaud@unistra.fr}}
	}



\begin{document}

\maketitle

\begin{abstract}
  The pay as you go model of  cloud operators makes exceeding the arbitrary time
  limits of the payment model costly. Budgeting the cost of running a scientific
  workflow  requires users  to  be able  to reliably  predict  runtimes of  jobs
  within. Attempts  to do so  are hampered by  the heterogeneity and  opacity of
  cloud  platform   which  make   job  runtimes  both   variable  and   hard  to
  predict. Variable jobs call for a  form of stochastic simulation, which can be
  achieved  through  either complex  calculations  heavily  constrained to  some
  reference cases or  through a Monte-Carlo approach  iterating on deterministic
  simulations. In this paper we study  the limits of the Monte-Carlo approach by
  characterising the relationship  between the precision of  the input variables
  and the simulations results, as well as the impact of the number of iterations
  for each level of input precision. We show.
\end{abstract}

\begin{IEEEkeywords}
cloud computing, infrastructure as a service, simulation, montecarlo.
\end{IEEEkeywords}

\section{Introduction.}

Whether to  model impractical  experiments or extract  information out  of large
quantities of data, large scale computing is central to scientific, and sometime
industrial,  operations.  Institutions  have  historically taken  the burden  of
providing the computing power necessary for the research of its members, usually
through  the acquisition  of a  cluster  or the  formation  of a  grid with  its
existing resources, sometime pooling resource  with other institution to achieve
higher computing power.  These resources are made available freely to members of
the institutions within the constraints  of time sharing and available computing
power.

Over the  last decade the advancement  of virtiualization techniques has  lead to
the emergence of new economic and exploitation approach of computer resources in
the form of \ac{IaaS}. In this model, all computing resources are made available
on demand  by third-party operators  and payed based  on usage.  The  ability to
provision resources on demand provided by  \ac{IaaS} operators is mainly used in
two ways.  Firstly,  for scaling purposes where new machines  are brought online
to fulfill  service availability in  the face of  higher load, this  approach is
used for  providing service allows for  a lower baseline cost  while still being
able to deal  by spikes in demand  by provisioning machine on  the go.  Secondly
for parallelizing tasks to achieve shorter makespan as equal cost, this approach
is  used for  scientific and  industrial  workload with  a clear  end and  where
runtime is heavily dependent on computing power.  This approach is made possible
by   the   pricing   model   of  cloud   infrastructure,   as   popularized   by
AWS\footnote{Amazon  Web  Services},  in  which  payment  for  computing  power,
provided  as  \acp{VM},  happens  in  increment of  arbitrary  length  of  time,
\ac{BTU}, usually of one hour. Running two \ac{VM} side by side for one \ac{BTU}
each costs  the same  as running  one \ac{VM}  for two  \ac{BTU}, but  every BTU
started is  owed in full.  As  such within a  workflow a slowed job  forcing the
subsequent job to run beyond the \ac{BTU}  limit can cause a full \ac{BTU} to be
invoiced for a handful of seconds of computation. Cases where such a thing might
not  always  be  avoided  but  \ac{IaaS}  to  be  reliably  used  in  scientific
computations  the eventuality  of  an  overrun must  be  reliably predicted  and
budgeted.

Accurate  prediction of  the  runtime  of scientific  workloads  is hampered  by
multiple factors. First \ac{IaaS} operates in a opaque fashion, the exact nature
of the underlying platforms are unknown and extremely heterogeneous as operators
complete  their data-centers  over the  years  with new  servers and  equipment.
Secondly cloud systems  are multi-tenant by nature which ads  uncertainty due to
contention  on  network  and  memory  accesses, depending  on  how  \ac{VM}  are
scheduled  and  the activity  of  your  \emph{neighbors}.  Even  when  \ac{IaaS}
operators attempt  to mask these  irregularities in computing power  and network
access by guaranteeing a minimum  performance, variability occurs in presence of
\emph{less-than-noisy-neighbors} as it  is not in the interest  of the \ac{IaaS}
operators to  limit power  when available over  the guaranteed  minimums.  These
factor add  to the  already high  difficulty of modeling  job exaction  times on
Networks   of   Computers    as   shown   by   Lastovetsky    and   Twamley   in
\cite{Lastovetsky05}.

To deal with the inherent unpredictability introduced by opaque heterogeneous
grids, the standard approach is to consider jobs runtimes to be stochastic.
Every job can be modeled by a \ac{rv} that models the whole spectrum of
possible runtimes. These \ac{rv} are the basis required for a stochastic
simulations.  Such simulations output a random variable of the observed
phenomenon (\emph{makespan} or \emph{\ac{BTU}}) which in turn can be used to
create intervals of possible results with their assorted confidence. In this
paper we will evaluate the precision of Monte Carlo method based stochastic
simulators in the context of cloud scheduling.

\section{Related Works.}

Computing the resulting \ac{rv} of a stochastic simulation a non-trivial
process.  \acp{rv} are defined through their \ac{pdf} and \ac{cdf} where $CDF(x)
= \int_{-\infty}^{x} PDF(y) dy$. Works by \etal{Li}~\cite{Li97} and
\etal{Ludwig}\cite{Ludwig01} show that in the context of task \ac{dag} with
independent \acp{rv} the \ac{rv} representing successive tasks is a convolution
product of their respective \acp{pdf} whereas the \ac{rv} of parallel tasks
joining is the product of their respective \acp{cdf}. Solving stochastic
\ac{dag} numerically is therefore extremely computationally intensive even when
the initial constraint of independence of the \acp{rv} can be fulfilled. In the
general cases solving the \ac{rv} for a \ac{dag} is deemed a \#P-complete
problem, though some approximation can be used to attempt and solve it
numerically as in~\cite{dodin85}.

To sidestep the computational difficulty \emph{Beenhakker} suggests in
1963\cite{beenhakker63} the use of Monte Carlo methods. In Monte Carlo
simulations the random inputs are repeatedly sampled to compute multiple
results, those results are used to compute the process overall \acl{rv}. Because
it is based on a repetition of a deterministic simulation using samples drawn
for the input \ac{rv}, this method avoids complex operations such as
convolution or the repeated integration needed to switch from an \ac{rv}'s
\ac{pdf} to its \ac{cdf}. This approach was used in 1971 by~\etal{Burt} for
network analysis~\cite{burt71} and in the context of task scheduling by
\etal{Canon}~\cite{Canon10} to compare the robustness of \ac{dag} schedules, and
\etal{Zheng}, to optimise schedules stochastic \acp{dag}. \etal{Cai} have worked
to extend CloudSim~\cite{cloudsim} with ElasticSim, a Monte Carlo
simulator~\cite{cai16}. As Monte Carlo simulations gain in traction we want to
evaluate the quality of this approach relatively to the precision of the tasks
models.

\section{Problem description}

\bibliographystyle{IEEEtran}
\bibliography{montecarlo-simulation}

\newpage
\begin{verbatim}
* Difficulties of simulation
	- opaque platforms
	- difficulties simulation precision (aka opaque codes)(cite simgrid ?)
	- difficulties of stochastic simulation (cite robust DAG over possible approach)(cite elastic sim)
	- introduce Montecarlo (just lipservice)

! Related works
	- Robust DAG by Jeanot et al : does the full run around of stochastic simulation approach (back-cite the other ?) -> wanted to test weather scheduling was robust
	- Elastic sim use montecarlo processes also for DAG	
	- Stochatsict Dag Sechduling  Zheng et Sakellariou -> looks for the best scheduling in a runtime scheduling contexti
	? Simgrid
	- Schlouder
	? Towards a realistic performance model Lastovesky Rychkov -> but program execution variability.

! Problem description
	
* Montecarlo as a workaround 
	- Deterministic simulator can be turned stochastic by being run on a sample of stochastic inputs

* Deterministic simulators are easy to evaluate they have 1 correct result for any set of input 


* The stochastic component must be evaluated
	- incorrect sample of inputs in the simulator will lead to wrong results
	- how does the precision of the input affects the precision of the results
	- how does the precision of the input affects the montecarlo process (ie. num of simulations necessary and convergence speed)

! Methodology 

* Rerunning experiments and simulator validation
	- the schlouder Cloud batch scheduler
	- simschlouder and how it relates to schlouder
	- the experimental backlog (OMSSA/Montage and the platforms on with they ran

* the perfect model
	- model based on real run values
	- draw interval centered on real runtime

! Results
\end{verbatim}


\end{document}
