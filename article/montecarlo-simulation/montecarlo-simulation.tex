\documentclass[10pt,conference,compsocconf]{IEEEtran}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{tikz}

\usepackage{acronym}
\acrodef{IaaS}{Infrastructure as a Service}
\acrodef{SaaS}{Software as a Service}
\acrodef{PaaS}{Platform as a Service}
\acrodef{dag}[DAG]{directed acyclic graph}
\acrodef{VM}{Virtual Machine}
\acrodef{PM}{Physical Machine}
\acrodef{BTU}{billing time unit}
\acrodef{EC2CU}{EC2 Compute Unit}
\acrodef{HPC}{High Performance Computing}
\acrodef{unistra}{University of Strasbourg}
\acrodef{rv}[RV]{random variable}
\acrodef{pdf}[PDF]{probability density function}
\acrodef{cdf}[CDF]{cumulative distribution function}
\acrodef{mcs}[MCS]{Monte-Carlo simulation}
\acrodefplural{mcs}[MCS's]{Monte-Carlo simulations}

\newcommand*\rot{\rotatebox{90}}
\newcommand{\pmpc}[1]{$\pm#1\%$}
\newcommand{\etal}[1]{\emph{#1 et al.}}

\title{Modeling the accuracy of Monte-Carlo approach for Cloud based workflow simulations.}
\author{\IEEEauthorblockN{Luke~Bertot 
			and Stéphane~Genaud 
			and Julien~Gossa}
	\IEEEauthorblockA{Icube-ICPS --- UMR 7357, Univeristé de Strasbourg, CNRS\\
		P\^ole API Blvd S. Bant, 67400 Illkirch\\
		email: \url{lbertot@unistra.fr}, \url{gossa@unistra.fr}, \url{genaud@unistra.fr}}
	}



\begin{document}

\maketitle

\begin{abstract}
  The pay as you go model of  cloud operators makes exceeding the arbitrary time
  limits of the payment model costly. Budgeting the cost of running a scientific
  workflow  requires users  to  be able  to reliably  predict  runtimes of  jobs
  within. Attempts  to do so  are hampered by  the heterogeneity and  opacity of
  cloud  platform   which  make   job  runtimes  both   variable  and   hard  to
  predict. Variable jobs call for a  form of stochastic simulation, which can be
  achieved  through  either complex  calculations  heavily  constrained to  some
  reference cases or  through a Monte-Carlo approach  iterating on deterministic
  simulations. In this paper we study  the limits of the Monte-Carlo approach by
  characterising the relationship  between the precision of  the input variables
  and the simulations results, as well as the impact of the number of iterations
  for each level of input precision. We show.
\end{abstract}

\begin{IEEEkeywords}
cloud computing, infrastructure as a service, simulation, montecarlo.
\end{IEEEkeywords}

\section{Introduction.}

Whether to  model impractical  experiments or extract  information out  of large
quantities of data, large scale computing is central to scientific, and sometime
industrial,  operations.  Institutions  have  historically taken  the burden  of
providing the computing power necessary for the research of its members, usually
through  the acquisition  of a  cluster  or the  formation  of a  grid with  its
existing resources, sometime pooling resource  with other institution to achieve
higher computing power.  These resources are made available freely to members of
the institutions within the constraints  of time sharing and available computing
power.

Over the  last decade the advancement  of virtiualization techniques has  lead to
the emergence of new economic and exploitation approach of computer resources in
the form of \ac{IaaS}. In this model, all computing resources are made available
on demand  by third-party operators  and payed based  on usage.  The  ability to
provision resources on demand provided by  \ac{IaaS} operators is mainly used in
two ways.  Firstly,  for scaling purposes where new machines  are brought online
to fulfill  service availability in  the face of  higher load, this  approach is
used for  providing service allows for  a lower baseline cost  while still being
able to deal  by spikes in demand  by provisioning machine on  the go.  Secondly
for parallelizing tasks to achieve shorter makespan as equal cost, this approach
is  used for  scientific and  industrial  workload with  a clear  end and  where
runtime is heavily dependent on computing power.  This approach is made possible
by   the   pricing   model   of  cloud   infrastructure,   as   popularized   by
AWS\footnote{Amazon  Web  Services},  in  which  payment  for  computing  power,
provided  as  \acp{VM},  happens  in  increment of  arbitrary  length  of  time,
\ac{BTU}, usually of one hour. Running two \ac{VM} side by side for one \ac{BTU}
each costs  the same  as running  one \ac{VM}  for two  \ac{BTU}, but  every BTU
started is  owed in full.  As  such within a  workflow a slowed job  forcing the
subsequent job to run beyond the \ac{BTU}  limit can cause a full \ac{BTU} to be
invoiced for a handful of seconds of computation. Cases where such a thing might
not  always  be  avoided  but  \ac{IaaS}  to  be  reliably  used  in  scientific
computations  the eventuality  of  an  overrun must  be  reliably predicted  and
budgeted.

Accurate  prediction of  the  runtime  of scientific  workloads  is hampered  by
multiple factors. First \ac{IaaS} operates in a opaque fashion, the exact nature
of the underlying platforms are unknown and extremely heterogeneous as operators
complete  their data-centers  over the  years  with new  servers and  equipment.
Secondly cloud systems  are multi-tenant by nature which ads  uncertainty due to
contention  on  network  and  memory  accesses, depending  on  how  \ac{VM}  are
scheduled  and  the activity  of  your  \emph{neighbors}.  Even  when  \ac{IaaS}
operators attempt  to mask these  irregularities in computing power  and network
access by guaranteeing a minimum  performance, variability occurs in presence of
\emph{less-than-noisy-neighbors} as it  is not in the interest  of the \ac{IaaS}
operators to  limit power  when available over  the guaranteed  minimums.  These
factor add  to the  already high  difficulty of modeling  job exaction  times on
Networks of Computers as shown by \emph{Lastovetsky} and \emph{Twamley} in
\cite{Lastovetsky05}.

To deal with the inherent unpredictability introduced by opaque heterogeneous
grids, the standard approach is to consider jobs runtimes to be stochastic.
Every job can be modeled by a \ac{rv} that models the whole spectrum of
possible runtimes. These \ac{rv} are the basis required for a stochastic
simulations.  Such simulations output a random variable of the observed
phenomenon (\emph{makespan} or \emph{\ac{BTU}}) which in turn can be used to
create intervals of possible results with their assorted confidence. In this
paper we will evaluate the precision of Monte Carlo method based stochastic
simulators in the context of cloud scheduling.

\section{Related Works.}

Computing the resulting \ac{rv} of a stochastic simulation is a non-trivial
process.  \acp{rv} are defined through their \ac{pdf} and \ac{cdf} where $CDF(x)
= \int_{-\infty}^{x} PDF(y) dy$. Works by \etal{Li}~\cite{Li97} and
\etal{Ludwig}\cite{Ludwig01} show that in the context of task \ac{dag} with
independent \acp{rv} the \ac{rv} representing successive tasks is a convolution
product of their respective \acp{pdf} whereas the \ac{rv} of parallel tasks
joining is the product of their respective \acp{cdf}. Solving stochastic
\ac{dag} numerically is therefore extremely computationally intensive even when
the initial constraint of independence of the \acp{rv} can be fulfilled. In the
general cases solving the \ac{rv} for a \ac{dag} is deemed a \#P-complete
problem, though some approximation can be used to attempt and solve it
numerically as in~\cite{dodin85}.

To sidestep the computational difficulty \emph{van Slyke} suggests in
1963\cite{Slyke63} the use of Monte Carlo methods. In \acp{mcs} the random
inputs are repeatedly sampled to compute multiple results, those results are
used to compute the process overall \acl{rv}. Because it is based on a
repetition of a deterministic simulation using samples drawn for the input
\ac{rv}, this method avoids complex operations such as convolution or the
repeated integration needed to switch from an \ac{rv}'s \ac{pdf} to its
\ac{cdf}. This approach was used in 1971 by~\etal{Burt} for network
analysis~\cite{burt71} and in the context of task scheduling by
\etal{Canon}~\cite{Canon10} to compare the robustness of \ac{dag} schedules, and
\etal{Zheng}, to optimise schedules stochastic \acp{dag}. \etal{Cai} have worked
to extend CloudSim~\cite{cloudsim} with ElasticSim, a Monte Carlo
simulator~\cite{cai16}. As \acp{mcs} gain in traction we want to
evaluate the quality of this approach relatively to the precision of the tasks
models.

\section{Le fil du coeur du travail}
Punch line:
*  on sait dire comment le makespan est affecté par différents niveaux de
perturbations 
    + étant donné un niveau de perturbation, on sait encadrer le
      makespan dans un IC 95\% (tout ça en simulation)
    + cet encadrement converge très vite: 5 à 10 simulations MC suffisent

* on obtient des makespan simulés très proches de la réalité : montrer l'écart
relatif à la réalité. 
\section{Simschlouder}

SimGrid~\cite{simgrid} is a discrete event simulator aimed at studying a variety
of distributed systems, such as Grids, Clouds  or HPC systems. On top of SimGrid
we implemented  SimSchlouder, a  simulated implementation  of the  Schlouder job
broker~\cite{Michon17}.  Schlouder handles resource  provisioning as well  as job
scheduling. In Schlouder the number of  machines used and the scheduling of jobs
on these  machine is controled  through a strategy. SimSchlouder  implements two
strategies :
\begin{itemize}
\item  ASAP: schedules  jobs as  soon as  possible, provisioning  more resources
  unless a machine is already available.
\item AFAP: schedules  jobs to already available resource  unless doing so
      would overun previous reservation.
\end{itemize}

\section{Monte-Carlo Simulation}

Suppose an  application consisting of  $n$ jobs,  independent or organized  as a
workflow.   The  simulation engine  SimSchlouder  takes  as input  the  runtimes
$T_1, \ldots ,  T_n$ of the different  jobs as specified by  the user, schedules
the jobs and  finally outputs the makespan  $M$ of the whole  batch.  The user's
estimation of  jobs' runtimes might be  inaccurate, and we describe  hereafter a
method to characterize the impact of repeated imprecisions (on each job runtime)
on the  final makespan.  We  have developed a \ac{mcp}  tool (see
Figure~\ref{fig:mcprocess}) to compute a confidence interval around the makespan
produced   by   simulation.   Let   us   sketch   the   overall   process   (see
Fig~\ref{fig:mc-process}):
\begin{itemize} 
\item from  the set of user  specified runtimes $\{T_i\}$, the  system generates
  $s$ sets of perturbed runtimes  $\{T_i^1\}, \ldots, \{T_i^s\}$. Each perturbed
  runtime    is   a    random   value    uniformely   drawn    in   the    range
  $[T_i (1-P); T_i  (1+P)]$, where $P$ is called  the \emph{perturbation level},
  e.g  5\%. We  call such  each random  draw of  a set  of perturbed  runtimes a
  \emph{realization}.
\item a simulation  is run for each realization, producing  a makespan $M^i$ for
  each,
\item  a  normal  distribution  ${\cal N}(\mu,\sigma)$  is  fitted  on
		the different  makespan values.
\end{itemize}
\begin{figure}
	\centering
	\resizebox{0.5\textwidth}{!}{%
		\input{mc-process.tex}
		}
	\caption{Overview of the Monte-Carlo process : $500$ realisations are
	generated by drawing and adding a perturbation to each job of the
	user provided set, every simulation is then simulated, the resulting
	makespans are fitted into the final result. \label{fig:mcprocess}}
      \label{fig:mc-process}
\end{figure}
Fitting is done to a normal distribution because, in essence the makespan is the
sum  of the  runtimes of  the jobs  on the  critical path  of the  schedule.  To
measure the impact  of the runtime perturbation on the  makespan, we compute the
range $[\mu-2\sigma;\mu+2\sigma]$ (that is  a 95\% confidence interval) relative
to  the mean  $\mu$. 
% plus tard, on ? : For  comparison  purpose we  test this  process with  both
%strategies. 
We now  seek to answer the  following two questions: given  a perturbation
  level  of  the input  runtimes,  i)  what is  the  convergence  speed of  the
\ac{mcs} and  ii) what  is the  size of  the confidence  interval
resulting from the \ac{mcs}.

For this \ac{mcs}, we use  as user provided runtimes six base sets
of runtimes obtained  by running a Montage\cite{montage2009}  workflow on actual
clusters.  For every base set we generate 500 realizations, and each realization
receives an order number.  Every  realization is then simulated by SimSchlouder.
In order  to study the  properties like convergence speed  we opted to  keep the
results of each  individual simulation indexed by base  set, perturbation level,
and  order number.   Normal distribution  is fitted  to data  using the  maximum
likelihood method.

This process was repeated at 4 different perturbation levels, \pmpc{5},
\pmpc{10}, \pmpc{15} and, \pmpc{20}.

\subsection{Monte Carlo Convergence}

Since Monte Carlo methods are based on repetition of random processes, it is
necessary to repeat the processes enough to obtain the intended result. In
simple Monte Carlo experiments, like integration, the relative error is capped
by $\frac{1}{\sqrt{n}}$, where $n$ is the number of samples, but complex applications
can require more samples or a more targeted sampling to yield correct
results. Our experiment includes both a non trivial simulation step, where
scheduling and job dependency affects which runtimes are significant, and a
strong aggregation step using normal law fitting. To determine the
convergence properties of our \ac{mcs} we observe the evolution
of the normal distribution obtained depending on the number of simulations done. 
Using the simulations order index we are able to produce the fitting we would
have obtained after any arbitrary number of simulation. For every subset we
observe the absolute relative error in the resulting values of $\mu$ and $\sigma$ 
when compared to the value taken when all 500 simulation results are used. 

Table~\ref{tab:mcs-convergence} how many simulations are required before the
absolute error consistently passes under the $.5\%$ threshold average makespan
value $\mu$ and the $1\%$ threshold of the quarter length of the confidence
interval $\sigma$ for different input perturbation levels. For the fitting
process to bear any value we started using at least 5 simulations. The result
show that for this type of simulations at reasonable level of perturbation $\mu$
converges quickly. A hundred simulation is generally enough to get a sense of
the workload makespan. On the other hand the computation of $\sigma$ is much
more volatile and does not appear to exhibit  as strong a convergence. This is
partly compensated by the fact the values of $\sigma$ small compared to the
values of $\mu$.

\begin{table}
	\centering
	%\caption{Monte Carlo simulation convergence based on perturbation}
	\begin{tabular}{rcc|cc|cc|cc}
		\cline{2-9}
		& \multicolumn{2}{c|}{$5\%$}& \multicolumn{2}{c|}{$10\%$}&
		\multicolumn{2}{c|}{$15\%$}& \multicolumn{2}{c}{$20\%$}\\
		&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}&\rot{$\mu.ae<.5\%$}&\rot{$\sigma.ae<1\%$}\\
		\hline
		min&$>5$&$402$&$>5$&$423$&$>5$&$318$&$>5$&$353$\\
		mean&$14$&$448$&$30$&$465$&$29$&$409$&$73$&$442$\\
		max&$44$&$498$&$88$&$490$&$71$&$480$&$238$&$497$\\
		\hline
	\end{tabular}
	\caption{Monte Carlo simulation convergence}
	\label{tab:mcs-convergence}
\end{table}

\subsection{Monte Carlo fitting process}

The fitting process used to aggregate the final result of the \ac{mcs} also 
possesses it's own confidence characteristic. The method we used
gives a standard deviation for the fitted value of $\mu$ and $\sigma$. Relative
standard deviation as a function of the number of the simulations are shown
Figure~\ref{fig:confidence}. We see here that the standard deviation measured on
$\mu$ and $\sigma$ converges in following a $\fract{1}{\sqrt{n}}$ pattern
common observed in Monte-Carlo methods. These observations comfort us in our
choice of fitting results to a Normal law since they show adding more samples
consistently reduce standard deviation on the results estimates. We also see the
higher perturbation levels reduce the confidence in the estimated value of $\mu$
but doesn't affect significantly the confidence in the estimates of $\sigma$.
This is balanced by the fact the standard deviation on the estimated of $\mu$
are is relatively small, whereas the standard deviation on the estimate of
$\sigma$, tho unaffected by the input perturbation is significant in comparison
to the value of $\sigma$.

\begin{figure}
	\centering
	\resizebox{0.5\textwidth}{!}{%
		\input{meanconf.tex}
	}
	\resizebox{0.5\textwidth}{!}{%
		\input{sdconf.tex}
	}
	\caption{Evolution of confidence on $\mu$ and $\sigma$ depending on the
	number of simulations.}
	\label{fig:confidence}
\end{figure}





\bibliographystyle{IEEEtran}
\bibliography{montecarlo-simulation}

\newpage
\begin{verbatim}
* Difficulties of simulation
	- opaque platforms
	- difficulties simulation precision (aka opaque codes)(cite simgrid ?)
	- difficulties of stochastic simulation (cite robust DAG over possible approach)(cite elastic sim)
	- introduce Montecarlo (just lipservice)

! Related works
	- Robust DAG by Jeanot et al : does the full run around of stochastic simulation approach (back-cite the other ?) -> wanted to test weather scheduling was robust
	- Elastic sim use montecarlo processes also for DAG	
	- Stochatsict Dag Sechduling  Zheng et Sakellariou -> looks for the best scheduling in a runtime scheduling contexti
	? Simgrid
	- Schlouder
	? Towards a realistic performance model Lastovesky Rychkov -> but program execution variability.

! Problem description
	
* Montecarlo as a workaround 
	- Deterministic simulator can be turned stochastic by being run on a sample of stochastic inputs

* Deterministic simulators are easy to evaluate they have 1 correct result for any set of input 


* The stochastic component must be evaluated
	- incorrect sample of inputs in the simulator will lead to wrong results
	- how does the precision of the input affects the precision of the results
	- how does the precision of the input affects the montecarlo process (ie. num of simulations necessary and convergence speed)

! Methodology 

* Rerunning experiments and simulator validation
	- the schlouder Cloud batch scheduler
	- simschlouder and how it relates to schlouder
	- the experimental backlog (OMSSA/Montage and the platforms on with they ran

* the perfect model
	- model based on real run values
	- draw interval centered on real runtime

! Results
\end{verbatim}


\end{document}
