In the cloud computing model, cloud providers invoice clients for resource consumption. Hence, tools helping the client to budget the cost of running his application are of pre-eminent importance. To that end, a number of cloud simulators have been proposed by researchers. However, the attempts to reach
reliable predictions are hampered by the opacity regarding the underlying hardware platform and the multi-tenant nature clouds make job runtimes both variable and hard to predict. In this paper, we investigate through two real use-cases, which parameters are the most influential in the prediction of the actual execution behavior in terms of cost and makespan.

We consider the execution of batch jobs on an actual platform, one example being a bag-of-tasks and the other one a workflow, whose jobs are scheduled using typical heuristics based on the user estimates of job runtimes. We propose an improved simulation framework based on the Monte-Carlo method to study the relationship between the precision of the user estimates and the accuracy of the simulation results regarding cost and makespan. Based on this stochastic process, predictions are expressed as interval-based makespan and cost. We show that most of the time imprecisions in user estimates are largely amortized in the final predictions but still can capture real observations. Finally, we offer evidence of the influence of the scheduling heuristics on the accuracy of the method. 
 
